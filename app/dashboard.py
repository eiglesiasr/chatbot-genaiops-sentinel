# app/dashboard.py
import mlflow
import pandas as pd
import streamlit as st

st.set_page_config(page_title="ðŸ“Š Dashboard General de EvaluaciÃ³n", layout="wide")
st.title("ðŸ“ˆ EvaluaciÃ³n Completa del Chatbot por Pregunta")

# âœ… Buscar todos los experimentos que comienzan con "eval_"
client = mlflow.tracking.MlflowClient()
experiments = [exp for exp in client.search_experiments() if exp.name.startswith("eval_")]

if not experiments:
    st.warning("No se encontraron experimentos de evaluaciÃ³n.")
    st.stop()

# Mostrar opciones
exp_names = [exp.name for exp in experiments]
selected_exp_name = st.selectbox("Selecciona un experimento para visualizar:", exp_names)

experiment = client.get_experiment_by_name(selected_exp_name)
print("--- Experiment ID seleccionado: ---")
print(experiment.experiment_id)
runs = client.search_runs(experiment_ids=[experiment.experiment_id], order_by=["start_time DESC"])
print("--- NÃºmero de runs encontrados: ---")
print(len(runs))

# Convertir runs a DataFrame
data = []
for run in runs:
    print("--- Procesando un run ---")
    params = run.data.params
    metrics = run.data.metrics
    row_data = {
        "pregunta": params.get("question"),
        "prompt_version": params.get("prompt_version"),
        "chunk_size": int(params.get("chunk_size", 0)),
        "chunk_overlap": int(params.get("chunk_overlap", 0)),
        "lc_is_correct": metrics.get("lc_is_correct", 0),
        "Coherence": metrics.get("coherence_score", 0),
        "Correctness": metrics.get("correctness_score", 0),
        "Harmfulness": metrics.get("harmfulness_score", 0),
        "Relevance": metrics.get("relevance_score", 0),
        "Toxicity": metrics.get("toxicity_score", 0)
    }
    data.append(row_data)
    print("--- Datos de una fila ---")
    print(row_data)

df = pd.DataFrame(data)
print("--- DataFrame Completo ---")
print(df)

# Calcular la precisiÃ³n global basada en el tipo de experimento
if "criteria" in selected_exp_name:
    numeric_criteria_metrics = df[["Coherence", "Correctness", "Harmfulness", "Relevance", "Toxicity"]]
    valid_criteria_rows = numeric_criteria_metrics[(numeric_criteria_metrics != 0).any(axis=1)]
    global_precision = valid_criteria_rows.mean(axis=1).mean() * 100 if not valid_criteria_rows.empty else 0.0
else:
    global_precision = df["lc_is_correct"].mean() * 100 if not df.empty else 0.0

# Mostrar la precisiÃ³n global y el nÃºmero de respuestas evaluadas
st.subheader("ðŸ“Š MÃ©tricas Clave")
col1, col2 = st.columns(2)
col1.metric("PrecisiÃ³n Global", f"{global_precision:.1f}%")
col2.metric("Respuestas Evaluadas", len(df))

# Mostrar tabla completa
st.subheader("ðŸ“‹ Resultados individuales por pregunta")
st.dataframe(df)

# AgrupaciÃ³n para anÃ¡lisis
grouped = df.groupby(["prompt_version", "chunk_size"]).agg(
    promedio_correcto=("lc_is_correct", "mean"),
    promedio_Coherence=("Coherence", "mean"),
    promedio_Correctness=("Correctness", "mean"),
    promedio_Harmfulness=("Harmfulness", "mean"),
    promedio_Relevance=("Relevance", "mean"),
    promedio_Toxicity=("Toxicity", "mean"),
    preguntas=("pregunta", "count")
).reset_index()

st.subheader("ðŸ“Š Resumen del desempeÃ±o")
st.dataframe(grouped)

# GrÃ¡fico
grouped["config"] = grouped["prompt_version"] + " | " + grouped["chunk_size"].astype(str)
st.bar_chart(grouped.set_index("config")[["promedio_correcto",
                                            "promedio_Coherence",
                                            "promedio_Correctness",
                                            "promedio_Harmfulness",
                                            "promedio_Relevance",
                                            "promedio_Toxicity"
                                            ]])
